{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd80e778",
   "metadata": {},
   "source": [
    "# Hybrid Forgery Training + Evaluation\n",
    "This notebook lets you tune every training hyperparameter, toggle ablation flags, and launch `run_training` with tqdm progress bars. After training, reuse the same configuration to load checkpoints, compute metrics (Dice/IoU/precision/recall/F1 + confusion matrix), and visualize 10 qualitative test samples with image / ground-truth / prediction / overlay columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/DeogenesMaranan/LIFD\"\n",
    "COLAB_REPO_DIR = Path(\"/content/LIFD\")\n",
    "\n",
    "\n",
    "def running_in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "USE_COLAB = running_in_colab()\n",
    "\n",
    "if USE_COLAB:\n",
    "    if COLAB_REPO_DIR.exists():\n",
    "        print(f\"Folder {COLAB_REPO_DIR} already exists. Deleting it...\")\n",
    "        shutil.rmtree(COLAB_REPO_DIR)\n",
    "\n",
    "    print(f\"Cloning {REPO_URL} -> {COLAB_REPO_DIR}\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(COLAB_REPO_DIR)], check=True)\n",
    "\n",
    "    os.chdir(COLAB_REPO_DIR)\n",
    "    print(f\"Working directory set to {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"Colab environment not detected; using current local working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69922e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_BASE_DIR = \"/content/drive/MyDrive/LIFD\"\n",
    "if USE_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "    except ModuleNotFoundError as exc:\n",
    "        raise RuntimeError(\"google.colab is not available. Set USE_COLAB=False to bypass Drive mounting.\") from exc\n",
    "else:\n",
    "    DRIVE_BASE_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd = None\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    requests = None\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download, snapshot_download\n",
    "except ImportError:\n",
    "    hf_hub_download = None\n",
    "    snapshot_download = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CPU_COUNT = os.cpu_count() or 4\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        _gpu_props = torch.cuda.get_device_properties(0)\n",
    "        GPU_TOTAL_MEM_GB = _gpu_props.total_memory / (1024 ** 3)\n",
    "    except Exception:\n",
    "        GPU_TOTAL_MEM_GB = None\n",
    "else:\n",
    "    GPU_TOTAL_MEM_GB = None\n",
    "\n",
    "env_perf_mode = os.environ.get(\"LIFD_PERF_MODE\", \"\").strip().lower()\n",
    "if env_perf_mode in {\"fast\", \"balanced\", \"throughput\"}:\n",
    "    PERFORMANCE_MODE = env_perf_mode\n",
    "else:\n",
    "    if GPU_AVAILABLE and CPU_COUNT >= 8:\n",
    "        PERFORMANCE_MODE = \"throughput\"\n",
    "    elif CPU_COUNT >= 6:\n",
    "        PERFORMANCE_MODE = \"balanced\"\n",
    "    else:\n",
    "        PERFORMANCE_MODE = \"fast\"\n",
    "\n",
    "if PERFORMANCE_MODE == \"throughput\":\n",
    "    DATA_WORKERS = max(4, min(8, CPU_COUNT - 1))\n",
    "    TRAIN_BATCH_SIZE = 32 if GPU_AVAILABLE else 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    PREFETCH_FACTOR = 6 if DATA_WORKERS > 0 else None\n",
    "elif PERFORMANCE_MODE == \"balanced\":\n",
    "    DATA_WORKERS = max(3, min(6, CPU_COUNT - 1))\n",
    "    TRAIN_BATCH_SIZE = 24 if GPU_AVAILABLE else 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    PREFETCH_FACTOR = 4 if DATA_WORKERS > 0 else None\n",
    "else:\n",
    "    half_cpus = max(1, CPU_COUNT // 2)\n",
    "    DATA_WORKERS = max(1, min(4, half_cpus))\n",
    "    TRAIN_BATCH_SIZE = 16 if GPU_AVAILABLE else 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    PREFETCH_FACTOR = 2 if DATA_WORKERS > 0 else None\n",
    "\n",
    "HEURISTIC_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "SAFE_BATCH_CAP = None\n",
    "if GPU_TOTAL_MEM_GB is not None:\n",
    "    if GPU_TOTAL_MEM_GB < 12:\n",
    "        SAFE_BATCH_CAP = 6\n",
    "    elif GPU_TOTAL_MEM_GB < 16:\n",
    "        SAFE_BATCH_CAP = 8\n",
    "    elif GPU_TOTAL_MEM_GB < 20:\n",
    "        SAFE_BATCH_CAP = 12\n",
    "\n",
    "if SAFE_BATCH_CAP is not None:\n",
    "    TRAIN_BATCH_SIZE = min(TRAIN_BATCH_SIZE, SAFE_BATCH_CAP)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    MICRO_BATCH_TARGET = 8\n",
    "    TRAIN_BATCH_SIZE = min(TRAIN_BATCH_SIZE, MICRO_BATCH_TARGET)\n",
    "    EFFECTIVE_TARGET_BATCH = 24\n",
    "else:\n",
    "    EFFECTIVE_TARGET_BATCH = TRAIN_BATCH_SIZE\n",
    "\n",
    "GRAD_ACCUM_STEPS = max(GRAD_ACCUM_STEPS, math.ceil(EFFECTIVE_TARGET_BATCH / max(TRAIN_BATCH_SIZE, 1)))\n",
    "\n",
    "LIGHTWEIGHT_MODEL = os.environ.get(\"LIFD_LIGHT_MODEL\", \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n",
    "\n",
    "if GPU_TOTAL_MEM_GB is not None:\n",
    "    print(f\"Detected GPU memory: {GPU_TOTAL_MEM_GB:.1f} GB\")\n",
    "if TRAIN_BATCH_SIZE != HEURISTIC_BATCH_SIZE:\n",
    "    print(\n",
    "        f\"Auto-adjusted batch size from {HEURISTIC_BATCH_SIZE} to {TRAIN_BATCH_SIZE} (grad_accum={GRAD_ACCUM_STEPS}).\"\n",
    "    )\n",
    "if GRAD_ACCUM_STEPS > 1:\n",
    "    effective_batch = TRAIN_BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "    print(f\"Effective batch size via accumulation: ~{effective_batch}\")\n",
    "if LIGHTWEIGHT_MODEL:\n",
    "    print(\"Lightweight model profile enabled (Swin backbone disabled, fused width reduced).\")\n",
    "else:\n",
    "    print(\"Full backbone profile enabled (EfficientNet + Swin + UNet).\")\n",
    "\n",
    "BASE_PATH = Path(DRIVE_BASE_DIR)\n",
    "\n",
    "from train import LossConfig, TrainConfig, run_training\n",
    "from model.hybrid_forgery_detector import HybridForgeryConfig\n",
    "from evaluation.eval_utils import (\n",
    "    collect_visual_samples,\n",
    "    evaluate_split,\n",
    "    load_model_from_checkpoint,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "DATASET_ROOT = Path(\"/content/data\") if USE_COLAB else Path(\"prepared\") / \"CASIA2\"\n",
    "DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HF_REPO_ID = \"juhenes/lifd\"\n",
    "HF_REVISION = os.environ.get(\"HF_DATA_REVISION\", \"main\")\n",
    "HF_TOKEN = userdata.get(\"HUGGINGFACE_TOKEN\") if userdata is not None else os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "MANIFEST_FILENAME = \"manifest.parquet\"\n",
    "\n",
    "def _locate_manifest_root(base: Path):\n",
    "    candidates = [base, base / \"CASIA2\"]\n",
    "    for candidate in candidates:\n",
    "        manifest_path = candidate / MANIFEST_FILENAME\n",
    "        if manifest_path.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def _has_complete_dataset(root: Path) -> bool:\n",
    "    return _locate_manifest_root(root) is not None\n",
    "\n",
    "def ensure_dataset_ready(force_sync: bool = False) -> Path:\n",
    "    if not force_sync:\n",
    "        local_root = _locate_manifest_root(DATASET_ROOT)\n",
    "        if local_root is not None:\n",
    "            return local_root\n",
    "    snapshot_dir = Path(snapshot_download(\n",
    "        repo_id=HF_REPO_ID,\n",
    "        revision=HF_REVISION,\n",
    "        token=HF_TOKEN,\n",
    "        repo_type=\"dataset\",\n",
    "    ))\n",
    "    resolved_snapshot = _locate_manifest_root(snapshot_dir)\n",
    "    if resolved_snapshot is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Downloaded dataset at {snapshot_dir} does not contain {MANIFEST_FILENAME}.\"\n",
    "        )\n",
    "    return resolved_snapshot\n",
    "\n",
    "DATASET_ROOT = ensure_dataset_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def inspect_prepared_dataset(root: Path, split: str = \"train\", preview: int = 5):\n",
    "    root = Path(root)\n",
    "    print(f\"Resolved DATASET_ROOT: {root}\")\n",
    "    manifest_path = root / \"manifest.parquet\"\n",
    "    print(f\"Manifest present: {manifest_path.exists()} ({manifest_path})\")\n",
    "    if not manifest_path.exists():\n",
    "        return\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        manifest_df = pd.read_parquet(manifest_path)\n",
    "        print(f\"Manifest rows: {len(manifest_df):,}\")\n",
    "        label_counts = manifest_df.groupby([\"split\"]).size().to_dict()\n",
    "        print(\"Rows per split:\", label_counts)\n",
    "    except Exception as exc:\n",
    "        print(f\"Manifest read failed: {exc}\")\n",
    "    split_dir = root / split\n",
    "    print(f\"Split directory exists: {split_dir.exists()} ({split_dir})\")\n",
    "    if not split_dir.exists():\n",
    "        return\n",
    "    shards = sorted(split_dir.glob(\"*.tar\"))\n",
    "    print(f\"Found {len(shards)} shard files under {split_dir}\")\n",
    "    for shard in islice(shards, preview):\n",
    "        print(f\" - {shard} | exists={shard.exists()} | size={shard.stat().st_size if shard.exists() else 'missing'}\")\n",
    "    missing = [str(p) for p in shards if not p.exists()]\n",
    "    if missing:\n",
    "        print(\"Missing shards:\", missing[:5], \"...\")\n",
    "\n",
    "inspect_prepared_dataset(Path(DATASET_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_cfg = TrainConfig()\n",
    "default_model_cfg = HybridForgeryConfig()\n",
    "print(\"Default TrainConfig:\")\n",
    "pprint(asdict(default_train_cfg))\n",
    "print(\"Default HybridForgeryConfig:\")\n",
    "pprint(asdict(default_model_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = LossConfig(\n",
    "    use_bce=True,\n",
    "    bce_weight=1.0,\n",
    "    use_dice=True,\n",
    "    dice_weight=1.0,\n",
    "    use_focal_tversky=True,\n",
    "    focal_tversky_weight=0.5,\n",
    "    focal_alpha=0.7,\n",
    "    focal_beta=0.3,\n",
    "    focal_gamma=1.2,\n",
    "    use_boundary=True,\n",
    "    boundary_weight=0.15,\n",
    "    apply_lovasz=True,\n",
    "    lovasz_weight=0.2,\n",
    ")\n",
    "loss_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(\n",
    "    prepared_root=str(DATASET_ROOT),\n",
    "    train_split=\"train\",\n",
    "    val_split=\"val\",\n",
    "    target_size=384,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    num_workers=DATA_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=DATA_WORKERS > 0,\n",
    "    pin_memory=True,\n",
    "    grad_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    grad_clip_norm=1.0,\n",
    "    log_interval=10,\n",
    "    checkpoint_dir=str(BASE_PATH / \"checkpoints\"),\n",
    "    checkpoint_interval=1,\n",
    "    save_best_only=False,\n",
    "    use_amp=True,\n",
    "    resume_from=str(BASE_PATH / \"checkpoints\" / \"epoch_001.pt\"),\n",
    "    loss_config=loss_config,\n",
    "    balance_real_fake=True,\n",
    "    balanced_positive_ratio=0.6,\n",
    "    eval_thresholds=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    primary_eval_threshold=0.5,\n",
    "    early_stopping_patience=6,\n",
    "    early_stopping_min_delta=5e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf70f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_kwargs = dict(\n",
    "    use_efficientnet=True,\n",
    "    use_swin=not LIGHTWEIGHT_MODEL,\n",
    "    use_segformer=False,\n",
    "    use_unet_decoder=True,\n",
    "    use_skip_connections=True,\n",
    "    pretrained_backbones=True,\n",
    "    fused_channels=192 if LIGHTWEIGHT_MODEL else 256,\n",
    "    use_noise_branch=True,\n",
    "    noise_branch_base_channels=32 if LIGHTWEIGHT_MODEL else 48,\n",
    "    noise_branch_num_stages=4,\n",
    "    noise_branch_use_residual=True,\n",
    "    noise_branch_use_high_pass=True,\n",
    "    backbone_input_size=train_config.target_size,\n",
    "    gradient_checkpointing=not LIGHTWEIGHT_MODEL,\n",
    ")\n",
    "\n",
    "train_config.model_config = HybridForgeryConfig(**model_config_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50613f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resolved device:\", train_config.resolved_device())\n",
    "print(\"TrainConfig overrides:\")\n",
    "pprint(asdict(train_config))\n",
    "print(\"HybridForgeryConfig overrides:\")\n",
    "pprint(asdict(train_config.model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c80516",
   "metadata": {},
   "source": [
    "### Optional Dry Run\n",
    "Flip the flag in the next cell to execute a 1-epoch, few-batch sanity check (uses `max_train_batches` / `max_val_batches`) before kicking off the full training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_DRY_RUN = False\n",
    "if ENABLE_DRY_RUN:\n",
    "    dry_run_config = deepcopy(train_config)\n",
    "    dry_run_config.num_epochs = 1\n",
    "    dry_run_config.batch_size = min(2, train_config.batch_size)\n",
    "    dry_run_config.max_train_batches = 1\n",
    "    dry_run_config.max_val_batches = 1\n",
    "    dry_run_config.checkpoint_dir = str(Path(train_config.checkpoint_dir) / \"dry_run\")\n",
    "    print(\"Dry run settings:\", {\n",
    "        \"num_epochs\": dry_run_config.num_epochs,\n",
    "        \"batch_size\": dry_run_config.batch_size,\n",
    "        \"max_train_batches\": dry_run_config.max_train_batches,\n",
    "        \"max_val_batches\": dry_run_config.max_val_batches,\n",
    "        \"checkpoint_dir\": dry_run_config.checkpoint_dir,\n",
    "    })\n",
    "    dry_run_history = run_training(dry_run_config)\n",
    "else:\n",
    "    print(\"Dry run skipped. Set ENABLE_DRY_RUN = True to execute the smoke test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67257c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "history = run_training(train_config)\n",
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lifd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
