{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd80e778",
   "metadata": {},
   "source": [
    "# Hybrid Forgery Training + Evaluation\n",
    "This notebook lets you tune every training hyperparameter, toggle ablation flags, and launch `run_training` with tqdm progress bars. After training, reuse the same configuration to load checkpoints, compute metrics (Dice/IoU/precision/recall/F1 + confusion matrix), and visualize 10 qualitative test samples with image / ground-truth / prediction / overlay columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/DeogenesMaranan/LIFD\"\n",
    "COLAB_REPO_DIR = Path(\"/content/LIFD\")\n",
    "\n",
    "\n",
    "def running_in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore  # noqa: F401\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "USE_COLAB = running_in_colab()\n",
    "\n",
    "if USE_COLAB:\n",
    "    if not COLAB_REPO_DIR.exists():\n",
    "        print(f\"Cloning {REPO_URL} -> {COLAB_REPO_DIR}\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(COLAB_REPO_DIR)], check=True)\n",
    "    else:\n",
    "        print(f\"Repository already exists at {COLAB_REPO_DIR}\")\n",
    "    os.chdir(COLAB_REPO_DIR)\n",
    "    print(f\"Working directory set to {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"Colab environment not detected; using current local working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69922e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_BASE_DIR = \"/content/drive/MyDrive/LIFD\"\n",
    "if USE_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "    except ModuleNotFoundError as exc:\n",
    "        raise RuntimeError(\"google.colab is not available. Set USE_COLAB=False to bypass Drive mounting.\") from exc\n",
    "else:\n",
    "    DRIVE_BASE_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd = None\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    requests = None\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "except ImportError:\n",
    "    hf_hub_download = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "CPU_COUNT = os.cpu_count() or 4\n",
    "DATA_WORKERS = max(2, min(8, CPU_COUNT - 1))\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "TRAIN_BATCH_SIZE = 32 if GPU_AVAILABLE else 8\n",
    "GRAD_ACCUM_STEPS = 1 if GPU_AVAILABLE else 2\n",
    "PREFETCH_FACTOR = 4 if DATA_WORKERS > 0 else None\n",
    "\n",
    "BASE_PATH = Path(DRIVE_BASE_DIR)\n",
    "if USE_COLAB:\n",
    "    DATASET_ROOT = Path(\"/content/data/CASIA2\")\n",
    "else:\n",
    "    DATASET_ROOT = Path(\"prepared\") / \"CASIA2\"\n",
    "\n",
    "HF_REPO_ID = \"juhenes/image-forgery-detection\"\n",
    "HF_FILE_NAME = \"CASIA2.zip\"\n",
    "HF_FILE_URL = None\n",
    "HF_TOKEN = userdata.get(\"HUGGINGFACE_TOKEN\") if userdata is not None else os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "def _download_hf_zip(dest_path: Path) -> Path:\n",
    "    if HF_FILE_URL:\n",
    "        if requests is None:\n",
    "            raise RuntimeError(\"requests is required to download from HF_FILE_URL.\")\n",
    "        headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"} if HF_TOKEN else None\n",
    "        response = requests.get(HF_FILE_URL, stream=True, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        with open(dest_path, \"wb\") as out_file:\n",
    "            for chunk in response.iter_content(chunk_size=1 << 20):\n",
    "                if chunk:\n",
    "                    out_file.write(chunk)\n",
    "        response.close()\n",
    "        return dest_path\n",
    "    if hf_hub_download is None:\n",
    "        raise RuntimeError(\"huggingface_hub is required to download CASIA2.zip when HF_FILE_URL is not provided.\")\n",
    "    return Path(\n",
    "        hf_hub_download(\n",
    "            repo_id=HF_REPO_ID,\n",
    "            filename=HF_FILE_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            repo_type=\"dataset\",\n",
    "            force_download=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def _extract_zip_flat(zip_path: Path, destination: Path) -> None:\n",
    "    with ZipFile(zip_path) as archive:\n",
    "        for member in archive.infolist():\n",
    "            member_path = Path(member.filename)\n",
    "            if not member_path.parts:\n",
    "                continue\n",
    "            if member_path.parts[0] == \"__MACOSX\":\n",
    "                continue\n",
    "            relative_parts = member_path.parts[1:] if len(member_path.parts) > 1 else member_path.parts\n",
    "            if not relative_parts:\n",
    "                continue\n",
    "            target_path = destination.joinpath(*relative_parts)\n",
    "            if member.is_dir():\n",
    "                target_path.mkdir(parents=True, exist_ok=True)\n",
    "                continue\n",
    "            target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with archive.open(member) as src, open(target_path, \"wb\") as dst:\n",
    "                shutil.copyfileobj(src, dst)\n",
    "\n",
    "\n",
    "def ensure_dataset_ready() -> None:\n",
    "    if not USE_COLAB:\n",
    "        return\n",
    "    DATASET_ROOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if DATASET_ROOT.exists() and any(DATASET_ROOT.iterdir()):\n",
    "        print(f\"Dataset already available at {DATASET_ROOT}\")\n",
    "        return\n",
    "    zip_file_path = DATASET_ROOT.parent / HF_FILE_NAME\n",
    "    downloaded_path = _download_hf_zip(zip_file_path)\n",
    "    if downloaded_path != zip_file_path:\n",
    "        shutil.copy(downloaded_path, zip_file_path)\n",
    "        downloaded_path = zip_file_path\n",
    "    DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    _extract_zip_flat(downloaded_path, DATASET_ROOT)\n",
    "    print(f\"Dataset extracted to {DATASET_ROOT}\")\n",
    "\n",
    "\n",
    "ensure_dataset_ready()\n",
    "\n",
    "from train import TrainConfig, run_training\n",
    "from model.hybrid_forgery_detector import HybridForgeryConfig\n",
    "from evaluation.eval_utils import (\n",
    "    collect_visual_samples,\n",
    "    evaluate_split,\n",
    "    load_model_from_checkpoint,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_cfg = TrainConfig()\n",
    "default_model_cfg = HybridForgeryConfig()\n",
    "print(\"Default TrainConfig:\")\n",
    "pprint(asdict(default_train_cfg))\n",
    "print(\"Default HybridForgeryConfig:\")\n",
    "pprint(asdict(default_model_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(\n",
    "    prepared_root=str(DATASET_ROOT),\n",
    "    train_split=\"train\",\n",
    "    val_split=\"val\",\n",
    "    target_size=128,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    num_workers=DATA_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=DATA_WORKERS > 0,\n",
    "    pin_memory=True,\n",
    "    grad_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    grad_clip_norm=1.0,\n",
    "    log_interval=10,\n",
    "    checkpoint_dir=str(BASE_PATH / \"checkpoints\"),\n",
    "    checkpoint_interval=1,\n",
    "    save_best_only=True,\n",
    "    use_amp=True,\n",
    "    resume_from=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf70f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config.model_config = HybridForgeryConfig(\n",
    "    use_efficientnet=True,\n",
    "    use_swin=True,\n",
    "    use_segformer=False,\n",
    "    use_unet_decoder=True,\n",
    "    use_skip_connections=True,\n",
    "    pretrained_backbones=True,\n",
    "    fused_channels=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50613f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resolved device:\", train_config.resolved_device())\n",
    "print(\"TrainConfig overrides:\")\n",
    "pprint(asdict(train_config))\n",
    "print(\"HybridForgeryConfig overrides:\")\n",
    "pprint(asdict(train_config.model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c80516",
   "metadata": {},
   "source": [
    "### Optional Dry Run\n",
    "Flip the flag in the next cell to execute a 1-epoch, few-batch sanity check (uses `max_train_batches` / `max_val_batches`) before kicking off the full training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_DRY_RUN = False\n",
    "if ENABLE_DRY_RUN:\n",
    "    dry_run_config = deepcopy(train_config)\n",
    "    dry_run_config.num_epochs = 1\n",
    "    dry_run_config.batch_size = min(2, train_config.batch_size)\n",
    "    dry_run_config.max_train_batches = 1\n",
    "    dry_run_config.max_val_batches = 1\n",
    "    dry_run_config.checkpoint_dir = str(Path(train_config.checkpoint_dir) / \"dry_run\")\n",
    "    print(\"Dry run settings:\", {\n",
    "        \"num_epochs\": dry_run_config.num_epochs,\n",
    "        \"batch_size\": dry_run_config.batch_size,\n",
    "        \"max_train_batches\": dry_run_config.max_train_batches,\n",
    "        \"max_val_batches\": dry_run_config.max_val_batches,\n",
    "        \"checkpoint_dir\": dry_run_config.checkpoint_dir,\n",
    "    })\n",
    "    dry_run_history = run_training(dry_run_config)\n",
    "else:\n",
    "    print(\"Dry run skipped. Set ENABLE_DRY_RUN = True to execute the smoke test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67257c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_training(train_config)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba09d27",
   "metadata": {},
   "source": [
    "## Evaluation, Samples, and Ablations\n",
    "Use the helpers below to load a checkpoint, compute aggregate metrics + confusion matrix on any split, and visualize 10 qualitative test samples aligned as image / ground-truth / prediction / overlay columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(train_config.checkpoint_dir) / \"best.pt\"\n",
    "evaluation_split = \"test\"\n",
    "ablation_label = f\"{checkpoint_path.stem}\"\n",
    "eval_device = train_config.resolved_device()\n",
    "max_eval_batches = None\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, trained_config = load_model_from_checkpoint(checkpoint_path, device=eval_device)\n",
    "evaluation_summary = evaluate_split(\n",
    "    model=model,\n",
    "    train_config=trained_config,\n",
    "    split=evaluation_split,\n",
    "    batch_size=trained_config.batch_size,\n",
    "    device=eval_device,\n",
    "    max_batches=max_eval_batches,\n",
    " )\n",
    "\n",
    "print(\"Aggregate metrics:\")\n",
    "pprint(evaluation_summary.metrics)\n",
    "print(\"\\nConfusion matrix (rows=actual clean/tampered, cols=predicted clean/tampered):\")\n",
    "if pd is not None:\n",
    "    display(pd.DataFrame(\n",
    "        evaluation_summary.confusion_matrix,\n",
    "        index=[\"Actual clean\", \"Actual tampered\"],\n",
    "        columns=[\"Pred clean\", \"Pred tampered\"],\n",
    "    ))\n",
    "else:\n",
    "    print(evaluation_summary.confusion_matrix)\n",
    "\n",
    "if \"ablation_results\" not in globals():\n",
    "    ablation_results = []\n",
    "\n",
    "ablation_results.append({\n",
    "    \"label\": ablation_label,\n",
    "    **evaluation_summary.metrics,\n",
    "})\n",
    "if pd is not None:\n",
    "    display(pd.DataFrame(ablation_results))\n",
    "else:\n",
    "    print(ablation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9acf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preview_samples = 10\n",
    "preview_samples = collect_visual_samples(\n",
    "    model=model,\n",
    "    train_config=trained_config,\n",
    "    split=evaluation_split,\n",
    "    num_samples=num_preview_samples,\n",
    "    device=eval_device,\n",
    " )\n",
    "\n",
    "columns = [\"image\", \"ground_truth\", \"prediction\", \"overlay\"]\n",
    "rows = len(preview_samples)\n",
    "if rows == 0:\n",
    "    raise RuntimeError(\"No samples with ground-truth masks were found in the requested split.\")\n",
    "fig, axes = plt.subplots(rows, len(columns), figsize=(15, 3 * rows))\n",
    "if rows == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "for row_idx, sample in enumerate(preview_samples):\n",
    "    for col_idx, key in enumerate(columns):\n",
    "        axes[row_idx, col_idx].imshow(sample[key], cmap=\"gray\" if key in {\"ground_truth\", \"prediction\"} else None)\n",
    "        axes[row_idx, col_idx].set_title(f\"{key.replace('_', ' ').title()} #{row_idx + 1}\")\n",
    "        axes[row_idx, col_idx].axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lifd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
