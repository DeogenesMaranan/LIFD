{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd80e778",
   "metadata": {},
   "source": [
    "# Hybrid Forgery Training + Evaluation\n",
    "This notebook lets you tune every training hyperparameter, toggle ablation flags, and launch `run_training` with tqdm progress bars. After training, reuse the same configuration to load checkpoints, compute metrics (Dice/IoU/precision/recall/F1 + confusion matrix), and visualize 10 qualitative test samples with image / ground-truth / prediction / overlay columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/DeogenesMaranan/LIFD\"\n",
    "COLAB_REPO_DIR = Path(\"/content/LIFD\")\n",
    "\n",
    "\n",
    "def running_in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "USE_COLAB = running_in_colab()\n",
    "\n",
    "if USE_COLAB:\n",
    "    if COLAB_REPO_DIR.exists():\n",
    "        print(f\"Folder {COLAB_REPO_DIR} already exists. Deleting it...\")\n",
    "        shutil.rmtree(COLAB_REPO_DIR)\n",
    "\n",
    "    print(f\"Cloning {REPO_URL} -> {COLAB_REPO_DIR}\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(COLAB_REPO_DIR)], check=True)\n",
    "\n",
    "    os.chdir(COLAB_REPO_DIR)\n",
    "    print(f\"Working directory set to {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"Colab environment not detected; using current local working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69922e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_BASE_DIR = \"/content/drive/MyDrive/LIFD\"\n",
    "if USE_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "    except ModuleNotFoundError as exc:\n",
    "        raise RuntimeError(\"google.colab is not available. Set USE_COLAB=False to bypass Drive mounting.\") from exc\n",
    "else:\n",
    "    DRIVE_BASE_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd = None\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    requests = None\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download, snapshot_download\n",
    "except ImportError:\n",
    "    hf_hub_download = None\n",
    "    snapshot_download = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CPU_COUNT = os.cpu_count() or 4\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        _gpu_props = torch.cuda.get_device_properties(0)\n",
    "        GPU_TOTAL_MEM_GB = _gpu_props.total_memory / (1024 ** 3)\n",
    "    except Exception:\n",
    "        GPU_TOTAL_MEM_GB = None\n",
    "else:\n",
    "    GPU_TOTAL_MEM_GB = None\n",
    "\n",
    "env_perf_mode = os.environ.get(\"LIFD_PERF_MODE\", \"\").strip().lower()\n",
    "if env_perf_mode in {\"fast\", \"balanced\", \"throughput\"}:\n",
    "    PERFORMANCE_MODE = env_perf_mode\n",
    "else:\n",
    "    if GPU_AVAILABLE and CPU_COUNT >= 8:\n",
    "        PERFORMANCE_MODE = \"throughput\"\n",
    "    elif CPU_COUNT >= 6:\n",
    "        PERFORMANCE_MODE = \"balanced\"\n",
    "    else:\n",
    "        PERFORMANCE_MODE = \"fast\"\n",
    "\n",
    "if PERFORMANCE_MODE == \"throughput\":\n",
    "    DATA_WORKERS = max(4, min(8, CPU_COUNT - 1))\n",
    "    TRAIN_BATCH_SIZE = 32 if GPU_AVAILABLE else 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    PREFETCH_FACTOR = 6 if DATA_WORKERS > 0 else None\n",
    "elif PERFORMANCE_MODE == \"balanced\":\n",
    "    DATA_WORKERS = max(3, min(6, CPU_COUNT - 1))\n",
    "    TRAIN_BATCH_SIZE = 24 if GPU_AVAILABLE else 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    PREFETCH_FACTOR = 4 if DATA_WORKERS > 0 else None\n",
    "else:\n",
    "    half_cpus = max(1, CPU_COUNT // 2)\n",
    "    DATA_WORKERS = max(1, min(4, half_cpus))\n",
    "    TRAIN_BATCH_SIZE = 16 if GPU_AVAILABLE else 8\n",
    "    GRAD_ACCUM_STEPS = 1\n",
    "    PREFETCH_FACTOR = 2 if DATA_WORKERS > 0 else None\n",
    "\n",
    "HEURISTIC_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "SAFE_BATCH_CAP = None\n",
    "if GPU_TOTAL_MEM_GB is not None:\n",
    "    if GPU_TOTAL_MEM_GB < 12:\n",
    "        SAFE_BATCH_CAP = 6\n",
    "    elif GPU_TOTAL_MEM_GB < 16:\n",
    "        SAFE_BATCH_CAP = 8\n",
    "    elif GPU_TOTAL_MEM_GB < 20:\n",
    "        SAFE_BATCH_CAP = 12\n",
    "\n",
    "if SAFE_BATCH_CAP is not None:\n",
    "    TRAIN_BATCH_SIZE = min(TRAIN_BATCH_SIZE, SAFE_BATCH_CAP)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    MICRO_BATCH_TARGET = 8\n",
    "    TRAIN_BATCH_SIZE = min(TRAIN_BATCH_SIZE, MICRO_BATCH_TARGET)\n",
    "    EFFECTIVE_TARGET_BATCH = 24\n",
    "else:\n",
    "    EFFECTIVE_TARGET_BATCH = TRAIN_BATCH_SIZE\n",
    "\n",
    "GRAD_ACCUM_STEPS = max(GRAD_ACCUM_STEPS, math.ceil(EFFECTIVE_TARGET_BATCH / max(TRAIN_BATCH_SIZE, 1)))\n",
    "\n",
    "LIGHTWEIGHT_MODEL = os.environ.get(\"LIFD_LIGHT_MODEL\", \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n",
    "\n",
    "if GPU_TOTAL_MEM_GB is not None:\n",
    "    print(f\"Detected GPU memory: {GPU_TOTAL_MEM_GB:.1f} GB\")\n",
    "if TRAIN_BATCH_SIZE != HEURISTIC_BATCH_SIZE:\n",
    "    print(\n",
    "        f\"Auto-adjusted batch size from {HEURISTIC_BATCH_SIZE} to {TRAIN_BATCH_SIZE} (grad_accum={GRAD_ACCUM_STEPS}).\"\n",
    "    )\n",
    "if GRAD_ACCUM_STEPS > 1:\n",
    "    effective_batch = TRAIN_BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "    print(f\"Effective batch size via accumulation: ~{effective_batch}\")\n",
    "if LIGHTWEIGHT_MODEL:\n",
    "    print(\"Lightweight model profile enabled (Swin backbone disabled, fused width reduced).\")\n",
    "else:\n",
    "    print(\"Full backbone profile enabled (EfficientNet + Swin + UNet).\")\n",
    "\n",
    "BASE_PATH = Path(DRIVE_BASE_DIR)\n",
    "\n",
    "from train import LossConfig, TrainConfig, run_training\n",
    "from model.hybrid_forgery_detector import HybridForgeryConfig\n",
    "from evaluation.eval_utils import (\n",
    "    collect_visual_samples,\n",
    "    evaluate_split,\n",
    "    load_model_from_checkpoint,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "DATASET_ROOT = Path(\"/content/data\") if USE_COLAB else Path(\"prepared\") / \"CASIA2\"\n",
    "DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HF_REPO_ID = \"juhenes/lifd\"\n",
    "HF_REVISION = os.environ.get(\"HF_DATA_REVISION\", \"main\")\n",
    "HF_TOKEN = userdata.get(\"HUGGINGFACE_TOKEN\") if userdata is not None else os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "MANIFEST_FILENAME = \"manifest.parquet\"\n",
    "\n",
    "def _locate_manifest_root(base: Path):\n",
    "    candidates = [base, base / \"CASIA2\"]\n",
    "    for candidate in candidates:\n",
    "        manifest_path = candidate / MANIFEST_FILENAME\n",
    "        if manifest_path.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def _has_complete_dataset(root: Path) -> bool:\n",
    "    return _locate_manifest_root(root) is not None\n",
    "\n",
    "def ensure_dataset_ready(force_sync: bool = False) -> Path:\n",
    "    if not force_sync:\n",
    "        local_root = _locate_manifest_root(DATASET_ROOT)\n",
    "        if local_root is not None:\n",
    "            return local_root\n",
    "    snapshot_dir = Path(snapshot_download(\n",
    "        repo_id=HF_REPO_ID,\n",
    "        revision=HF_REVISION,\n",
    "        token=HF_TOKEN,\n",
    "        repo_type=\"dataset\",\n",
    "    ))\n",
    "    resolved_snapshot = _locate_manifest_root(snapshot_dir)\n",
    "    if resolved_snapshot is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Downloaded dataset at {snapshot_dir} does not contain {MANIFEST_FILENAME}.\"\n",
    "        )\n",
    "    return resolved_snapshot\n",
    "\n",
    "DATASET_ROOT = ensure_dataset_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def inspect_prepared_dataset(root: Path, split: str = \"train\", preview: int = 5):\n",
    "    root = Path(root)\n",
    "    print(f\"Resolved DATASET_ROOT: {root}\")\n",
    "    manifest_path = root / \"manifest.parquet\"\n",
    "    print(f\"Manifest present: {manifest_path.exists()} ({manifest_path})\")\n",
    "    if not manifest_path.exists():\n",
    "        return\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        manifest_df = pd.read_parquet(manifest_path)\n",
    "        print(f\"Manifest rows: {len(manifest_df):,}\")\n",
    "        label_counts = manifest_df.groupby([\"split\"]).size().to_dict()\n",
    "        print(\"Rows per split:\", label_counts)\n",
    "    except Exception as exc:\n",
    "        print(f\"Manifest read failed: {exc}\")\n",
    "    split_dir = root / split\n",
    "    print(f\"Split directory exists: {split_dir.exists()} ({split_dir})\")\n",
    "    if not split_dir.exists():\n",
    "        return\n",
    "    shards = sorted(split_dir.glob(\"*.tar\"))\n",
    "    print(f\"Found {len(shards)} shard files under {split_dir}\")\n",
    "    for shard in islice(shards, preview):\n",
    "        print(f\" - {shard} | exists={shard.exists()} | size={shard.stat().st_size if shard.exists() else 'missing'}\")\n",
    "    missing = [str(p) for p in shards if not p.exists()]\n",
    "    if missing:\n",
    "        print(\"Missing shards:\", missing[:5], \"...\")\n",
    "\n",
    "inspect_prepared_dataset(Path(DATASET_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_cfg = TrainConfig()\n",
    "default_model_cfg = HybridForgeryConfig()\n",
    "print(\"Default TrainConfig:\")\n",
    "pprint(asdict(default_train_cfg))\n",
    "print(\"Default HybridForgeryConfig:\")\n",
    "pprint(asdict(default_model_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = LossConfig(\n",
    "    use_bce=True,\n",
    "    bce_weight=1.0,\n",
    "    use_dice=True,\n",
    "    dice_weight=1.0,\n",
    "    use_focal_tversky=True,\n",
    "    focal_tversky_weight=0.5,\n",
    "    focal_alpha=0.7,\n",
    "    focal_beta=0.3,\n",
    "    focal_gamma=1.2,\n",
    "    use_boundary=True,\n",
    "    boundary_weight=0.15,\n",
    "    apply_lovasz=True,\n",
    "    lovasz_weight=0.2,\n",
    ")\n",
    "loss_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(\n",
    "    prepared_root=str(DATASET_ROOT),\n",
    "    train_split=\"train\",\n",
    "    val_split=\"val\",\n",
    "    target_size=320,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-2,\n",
    "    num_workers=DATA_WORKERS,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    "    persistent_workers=DATA_WORKERS > 0,\n",
    "    pin_memory=True,\n",
    "    grad_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    grad_clip_norm=1.0,\n",
    "    log_interval=10,\n",
    "    checkpoint_dir=str(BASE_PATH / \"checkpoints\"),\n",
    "    checkpoint_interval=1,\n",
    "    save_best_only=True,\n",
    "    use_amp=True,\n",
    "    resume_from=None,\n",
    "    loss_config=loss_config,\n",
    "    balance_real_fake=True,\n",
    "    balanced_positive_ratio=0.6,\n",
    "    eval_thresholds=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    primary_eval_threshold=0.5,\n",
    "    early_stopping_patience=6,\n",
    "    early_stopping_min_delta=5e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf70f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_kwargs = dict(\n",
    "    use_efficientnet=True,\n",
    "    use_swin=not LIGHTWEIGHT_MODEL,\n",
    "    use_segformer=False,\n",
    "    use_unet_decoder=True,\n",
    "    use_skip_connections=True,\n",
    "    pretrained_backbones=True,\n",
    "    fused_channels=192 if LIGHTWEIGHT_MODEL else 256,\n",
    "    use_noise_branch=True,\n",
    "    noise_branch_base_channels=32 if LIGHTWEIGHT_MODEL else 48,\n",
    "    noise_branch_num_stages=4,\n",
    "    noise_branch_use_residual=True,\n",
    "    noise_branch_use_high_pass=True,\n",
    "    backbone_input_size=train_config.target_size,\n",
    "    gradient_checkpointing=not LIGHTWEIGHT_MODEL,\n",
    ")\n",
    "\n",
    "train_config.model_config = HybridForgeryConfig(**model_config_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50613f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resolved device:\", train_config.resolved_device())\n",
    "print(\"TrainConfig overrides:\")\n",
    "pprint(asdict(train_config))\n",
    "print(\"HybridForgeryConfig overrides:\")\n",
    "pprint(asdict(train_config.model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c80516",
   "metadata": {},
   "source": [
    "### Optional Dry Run\n",
    "Flip the flag in the next cell to execute a 1-epoch, few-batch sanity check (uses `max_train_batches` / `max_val_batches`) before kicking off the full training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_DRY_RUN = False\n",
    "if ENABLE_DRY_RUN:\n",
    "    dry_run_config = deepcopy(train_config)\n",
    "    dry_run_config.num_epochs = 1\n",
    "    dry_run_config.batch_size = min(2, train_config.batch_size)\n",
    "    dry_run_config.max_train_batches = 1\n",
    "    dry_run_config.max_val_batches = 1\n",
    "    dry_run_config.checkpoint_dir = str(Path(train_config.checkpoint_dir) / \"dry_run\")\n",
    "    print(\"Dry run settings:\", {\n",
    "        \"num_epochs\": dry_run_config.num_epochs,\n",
    "        \"batch_size\": dry_run_config.batch_size,\n",
    "        \"max_train_batches\": dry_run_config.max_train_batches,\n",
    "        \"max_val_batches\": dry_run_config.max_val_batches,\n",
    "        \"checkpoint_dir\": dry_run_config.checkpoint_dir,\n",
    "    })\n",
    "    dry_run_history = run_training(dry_run_config)\n",
    "else:\n",
    "    print(\"Dry run skipped. Set ENABLE_DRY_RUN = True to execute the smoke test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67257c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "history = run_training(train_config)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba09d27",
   "metadata": {},
   "source": [
    "## Evaluation, Samples, and Ablations\n",
    "Use the helpers below to load a checkpoint, compute aggregate metrics + confusion matrix on any split, and visualize 10 qualitative test samples aligned as image / ground-truth / prediction / overlay columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_auxiliary_tensors(npz_path: Path, device: torch.device, spatial_hw: tuple[int, int]):\n",
    "    with np.load(npz_path, allow_pickle=True) as data:\n",
    "        noise_inputs = {}\n",
    "        for key in (\"residual\", \"high_pass\"):\n",
    "            if key in data.files:\n",
    "                tensor = torch.from_numpy(data[key].transpose(2, 0, 1)).unsqueeze(0)\n",
    "                noise_inputs[key] = tensor.to(device)\n",
    "        if not noise_inputs:\n",
    "            h, w = spatial_hw\n",
    "            noise_inputs[\"residual\"] = torch.zeros(1, 3, h, w, device=device)\n",
    "        return torch.from_numpy(data[\"image\"].transpose(2, 0, 1)).unsqueeze(0).to(device), noise_inputs\n",
    "\n",
    "\n",
    "def predict_single_npz(checkpoint_path: Path, npz_path: Path, threshold: float = 0.5):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    cfg = HybridForgeryConfig(**checkpoint[\"config\"][\"model_config\"])\n",
    "    model = HybridForgeryDetector(cfg).to(train_config.resolved_device())\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    image, noise_inputs = load_auxiliary_tensors(npz_path, train_config.resolved_device(), spatial_hw=(train_config.target_size, train_config.target_size))\n",
    "    with torch.no_grad():\n",
    "        mask = model.predict_mask(image, threshold=threshold, noise_features=noise_inputs)\n",
    "    return mask.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(train_config.checkpoint_dir) / \"best.pt\"\n",
    "evaluation_split = \"test\"\n",
    "ablation_label = f\"{checkpoint_path.stem}\"\n",
    "eval_device = train_config.resolved_device()\n",
    "max_eval_batches = None\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, trained_config = load_model_from_checkpoint(checkpoint_path, device=eval_device)\n",
    "auto_threshold = True\n",
    "threshold_candidates = trained_config.eval_thresholds or [0.5]\n",
    "threshold_metric = \"f1\"\n",
    "evaluation_summary = evaluate_split(\n",
    "    model=model,\n",
    "    train_config=trained_config,\n",
    "    split=evaluation_split,\n",
    "    batch_size=trained_config.batch_size,\n",
    "    device=eval_device,\n",
    "    max_batches=max_eval_batches,\n",
    "    auto_threshold=auto_threshold,\n",
    "    threshold_candidates=threshold_candidates,\n",
    "    threshold_metric=threshold_metric,\n",
    ")\n",
    "\n",
    "best_threshold = evaluation_summary.metrics.get(\"best_threshold\", {}).get(\"value\", threshold_candidates[0])\n",
    "print(\"Aggregate metrics (threshold {:.2f}):\".format(best_threshold))\n",
    "pprint({k: v for k, v in evaluation_summary.metrics.items() if k in {\"loss\", \"dice\", \"iou\", \"precision\", \"recall\", \"f1\"}})\n",
    "print(\"\\nConfusion matrix (rows=actual clean/tampered, cols=predicted clean/tampered):\")\n",
    "if pd is not None:\n",
    "    display(pd.DataFrame(\n",
    "        evaluation_summary.confusion_matrix,\n",
    "        index=[\"Actual clean\", \"Actual tampered\"],\n",
    "        columns=[\"Pred clean\", \"Pred tampered\"],\n",
    "    ))\n",
    "else:\n",
    "    print(evaluation_summary.confusion_matrix)\n",
    "\n",
    "threshold_table = evaluation_summary.metrics.get(\"thresholds\", {})\n",
    "if pd is not None and threshold_table:\n",
    "    display(pd.DataFrame(threshold_table).T.sort_index())\n",
    "print(\"Best threshold ({}): {:.2f}\".format(threshold_metric, best_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9acf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preview_samples = 10\n",
    "preview_threshold = best_threshold if \"best_threshold\" in locals() else 0.5\n",
    "preview_samples = collect_visual_samples(\n",
    "    model=model,\n",
    "    train_config=trained_config,\n",
    "    split=evaluation_split,\n",
    "    num_samples=num_preview_samples,\n",
    "    device=eval_device,\n",
    "    threshold=preview_threshold,\n",
    ")\n",
    "\n",
    "columns = [\"image\", \"ground_truth\", \"prediction\", \"overlay\"]\n",
    "rows = len(preview_samples)\n",
    "if rows == 0:\n",
    "    raise RuntimeError(\"No samples with ground-truth masks were found in the requested split.\")\n",
    "fig, axes = plt.subplots(rows, len(columns), figsize=(15, 3 * rows))\n",
    "if rows == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "for row_idx, sample in enumerate(preview_samples):\n",
    "    for col_idx, key in enumerate(columns):\n",
    "        axes[row_idx, col_idx].imshow(sample[key], cmap=\"gray\" if key in {\"ground_truth\", \"prediction\"} else None)\n",
    "        axes[row_idx, col_idx].set_title(f\"{key.replace('_', ' ').title()} #{row_idx + 1}\")\n",
    "        axes[row_idx, col_idx].axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c5edc",
   "metadata": {},
   "source": [
    "### Single Image Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image, ImageOps\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "except ImportError:\n",
    "    colab_files = None\n",
    "    \n",
    "def _predict_image_array(image_arr: np.ndarray, checkpoint_path: Path, threshold: float) -> np.ndarray:\n",
    "    from model.hybrid_forgery_detector import HybridForgeryDetector\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    cfg = HybridForgeryConfig(**checkpoint[\"config\"][\"model_config\"])\n",
    "    device = train_config.resolved_device()\n",
    "    model = HybridForgeryDetector(cfg).to(device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    model.eval()\n",
    "    tensor = torch.from_numpy(image_arr.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "    zero_noise = torch.zeros(1, 3, tensor.shape[-2], tensor.shape[-1], device=device)\n",
    "    noise_inputs = {\"residual\": zero_noise}\n",
    "    with torch.no_grad():\n",
    "        mask = model.predict_mask(tensor, threshold=threshold, noise_features=noise_inputs)\n",
    "    return mask.squeeze().cpu().numpy()\n",
    "    \n",
    "def _load_uploaded_image() -> tuple[Image.Image, str]:\n",
    "    if USE_COLAB and colab_files is not None:\n",
    "        uploaded = colab_files.upload()\n",
    "        if not uploaded:\n",
    "            raise ValueError(\"No file uploaded. Please select an image.\")\n",
    "        name, data = next(iter(uploaded.items()))\n",
    "        return Image.open(BytesIO(data)).convert(\"RGB\"), name\n",
    "    local_image_path = Path(\"./REPLACE_WITH_IMAGE.jpg\")\n",
    "    if \"REPLACE_WITH_IMAGE\" in str(local_image_path):\n",
    "        raise ValueError(\"Set local_image_path to an actual image path when running outside Colab.\")\n",
    "    if not local_image_path.exists():\n",
    "        raise FileNotFoundError(f\"Local image not found: {local_image_path}\")\n",
    "    return Image.open(local_image_path).convert(\"RGB\"), str(local_image_path)\n",
    "    \n",
    "uploaded_image, uploaded_label = _load_uploaded_image()\n",
    "original_image = uploaded_image.copy()\n",
    "try:\n",
    "    resample_mode = Image.Resampling.BICUBIC\n",
    "except AttributeError:\n",
    "    resample_mode = Image.BICUBIC\n",
    "target_hw = (train_config.target_size, train_config.target_size)\n",
    "resized_image = ImageOps.fit(uploaded_image, target_hw, method=resample_mode)\n",
    "normalized = np.array(resized_image, dtype=np.float32) / 255.0\n",
    "checkpoint_for_single = checkpoint_path if \"checkpoint_path\" in locals() else Path(train_config.checkpoint_dir) / \"best.pt\"\n",
    "threshold_for_single = (\n",
    "    best_threshold if \"best_threshold\" in locals()\n",
    "    else (trained_config.primary_eval_threshold if \"trained_config\" in locals() else train_config.primary_eval_threshold)\n",
    " )\n",
    "pred_mask = _predict_image_array(normalized, checkpoint_for_single, threshold_for_single)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(f\"Original ({uploaded_label})\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(resized_image)\n",
    "axes[1].set_title(\"Resized\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(pred_mask, cmap=\"magma\")\n",
    "axes[2].set_title(f\"Prediction (thr={threshold_for_single:.2f})\")\n",
    "axes[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "pred_mask.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lifd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
